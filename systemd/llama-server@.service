# Systemd template service for llama.cpp server
# Install to: ~/.config/systemd/user/llama-server@.service (user) or /etc/systemd/system/ (system)
#
# Usage:
#   systemctl --user enable llama-server@qwen3-coder
#   systemctl --user start llama-server@qwen3-coder
#
# The instance name (%i) is used as the model config name
# Config files should be in ~/workspaces/local-llama/configs/%i.conf

[Unit]
Description=LLaMA Server (%i)
Documentation=https://github.com/ggerganov/llama.cpp
After=network.target
Wants=network-online.target

[Service]
Type=simple

# ROCm/HIP environment for AMD GPUs
Environment="HSA_OVERRIDE_GFX_VERSION=11.5.1"
Environment="HIP_VISIBLE_DEVICES=0"
Environment="GPU_MAX_HW_QUEUES=8"
Environment="LD_LIBRARY_PATH=/opt/rocm/lib:/usr/lib/x86_64-linux-gnu"

# UMA activation for APUs
Environment="GGML_CUDA_ENABLE_UNIFIED_MEMORY=1"
Environment="ROCBLAS_USE_HIPBLASLT=1"

# Memory settings
Environment="HSA_ENABLE_SDMA=0"
Environment="GPU_MAX_HEAP_SIZE=99"
Environment="GPU_MAX_ALLOC_PERCENT=99"

# Load instance-specific config
EnvironmentFile=%h/workspaces/local-llama/configs/%i.conf

# Default values (override in config file)
Environment="EXTRA_FLAGS="

ExecStart=/bin/bash -c '${LLAMA_BIN} --model ${MODEL_PATH} --host ${HOST} --port ${PORT} --ctx-size ${CTX_SIZE} --n-gpu-layers ${N_GPU_LAYERS} ${EXTRA_FLAGS}'

# Restart on failure
Restart=on-failure
RestartSec=10
StartLimitBurst=3
StartLimitIntervalSec=60

# Resource limits
LimitNOFILE=65536
LimitMEMLOCK=infinity

# Logging
StandardOutput=journal
StandardError=journal
SyslogIdentifier=llama-server-%i

[Install]
WantedBy=default.target
