{
  description = "Local LLM infrastructure for AMD APU/GPU with ROCm acceleration";

  inputs = {
    nixpkgs.url = "github:NixOS/nixpkgs/nixos-unstable";
    flake-utils.url = "github:numtide/flake-utils";
  };

  outputs =
    {
      self,
      nixpkgs,
      flake-utils,
    }:
    let
      # Supported systems (primarily Linux with ROCm)
      supportedSystems = [ "x86_64-linux" ];

      # Default configuration
      defaultConfig = {
        rocm = {
          gpuTarget = "gfx1151"; # Strix Halo APU
          version = "7.1.1";
          umaEnabled = true; # Unified Memory for APUs
        };

        models = {
          chat = {
            name = "qwen3-coder";
            port = 8000;
            contextSize = 262144;
            parallelSlots = 1;
          };
          embed = {
            name = "qwen3-embed";
            port = 8001;
            contextSize = 8192;
            parallelSlots = 4;
          };
          rerank = {
            name = "bge-reranker";
            port = 8002;
            contextSize = 512;
            parallelSlots = 4;
          };
        };

        proxy = {
          type = "litellm"; # "nginx" or "litellm"
          port = 4000; # LiteLLM default port
          tlsPort = 8443;
        };
      };
    in
    flake-utils.lib.eachSystem supportedSystems (
      system:
      let
        pkgs = import nixpkgs {
          inherit system;
          config.allowUnfree = true;
        };

        # ROCm environment variables
        rocmEnv = {
          HSA_OVERRIDE_GFX_VERSION = "11.5.1";
          HIP_VISIBLE_DEVICES = "0";
          GPU_MAX_HW_QUEUES = "8";
          LD_LIBRARY_PATH = "/opt/rocm/lib:/usr/lib/x86_64-linux-gnu";
          GGML_CUDA_ENABLE_UNIFIED_MEMORY = "1";
          ROCBLAS_USE_HIPBLASLT = "1";
          HSA_ENABLE_SDMA = "0";
          GPU_MAX_HEAP_SIZE = "99";
          GPU_MAX_ALLOC_PERCENT = "99";
        };

        # Generate systemd service for a model
        makeModelService =
          name: cfg:
          pkgs.writeText "llama-server-${name}.service" ''
            [Unit]
            Description=LLaMA Server (${name})
            Documentation=https://github.com/ggerganov/llama.cpp
            After=network.target
            Wants=network-online.target

            [Service]
            Type=simple
            User=%u
            Group=%u

            ${builtins.concatStringsSep "\n" (
              builtins.attrValues (builtins.mapAttrs (k: v: "Environment=\"${k}=${v}\"") rocmEnv)
            )}

            EnvironmentFile=/etc/llama-server/${name}.conf
            Environment="EXTRA_FLAGS="

            ExecStart=/bin/bash -c "''${LLAMA_BIN} --model ''${MODEL_PATH} --host ''${HOST} --port ''${PORT} --ctx-size ''${CTX_SIZE} --n-gpu-layers ''${N_GPU_LAYERS} ''${EXTRA_FLAGS}"

            Restart=on-failure
            RestartSec=10
            StartLimitBurst=3
            StartLimitIntervalSec=60

            LimitNOFILE=65536
            LimitMEMLOCK=infinity

            StandardOutput=journal
            StandardError=journal
            SyslogIdentifier=llama-server-${name}

            [Install]
            WantedBy=multi-user.target
          '';

        # LiteLLM configuration generator
        makeLiteLLMConfig =
          cfg:
          pkgs.writeText "litellm-config.yaml" ''
            # LiteLLM Proxy Configuration
            # Generated by local-llama flake

            model_list:
              # Chat/Completions model - aliased as common OpenAI models
              - model_name: gpt-4
                litellm_params:
                  model: openai/${cfg.models.chat.name}
                  api_base: http://localhost:${toString cfg.models.chat.port}/v1
                  api_key: "sk-local"

              - model_name: gpt-4-turbo
                litellm_params:
                  model: openai/${cfg.models.chat.name}
                  api_base: http://localhost:${toString cfg.models.chat.port}/v1
                  api_key: "sk-local"

              - model_name: gpt-3.5-turbo
                litellm_params:
                  model: openai/${cfg.models.chat.name}
                  api_base: http://localhost:${toString cfg.models.chat.port}/v1
                  api_key: "sk-local"

              - model_name: ${cfg.models.chat.name}
                litellm_params:
                  model: openai/${cfg.models.chat.name}
                  api_base: http://localhost:${toString cfg.models.chat.port}/v1
                  api_key: "sk-local"

              # Embeddings model
              - model_name: text-embedding-3-small
                litellm_params:
                  model: openai/${cfg.models.embed.name}
                  api_base: http://localhost:${toString cfg.models.embed.port}/v1
                  api_key: "sk-local"

              - model_name: text-embedding-3-large
                litellm_params:
                  model: openai/${cfg.models.embed.name}
                  api_base: http://localhost:${toString cfg.models.embed.port}/v1
                  api_key: "sk-local"

              - model_name: text-embedding-ada-002
                litellm_params:
                  model: openai/${cfg.models.embed.name}
                  api_base: http://localhost:${toString cfg.models.embed.port}/v1
                  api_key: "sk-local"

              - model_name: ${cfg.models.embed.name}
                litellm_params:
                  model: openai/${cfg.models.embed.name}
                  api_base: http://localhost:${toString cfg.models.embed.port}/v1
                  api_key: "sk-local"

              # Reranker model
              - model_name: rerank-english-v3.0
                litellm_params:
                  model: openai/${cfg.models.rerank.name}
                  api_base: http://localhost:${toString cfg.models.rerank.port}/v1
                  api_key: "sk-local"

              - model_name: ${cfg.models.rerank.name}
                litellm_params:
                  model: openai/${cfg.models.rerank.name}
                  api_base: http://localhost:${toString cfg.models.rerank.port}/v1
                  api_key: "sk-local"

            litellm_settings:
              # Enable caching (critical for embeddings)
              cache: true
              cache_params:
                type: "local"  # Use "redis" for persistent cache
                ttl: 3600      # Cache TTL in seconds

              # Request settings
              request_timeout: 600

              # Telemetry
              success_callback: []
              failure_callback: []

            general_settings:
              master_key: "sk-local-llm-master"  # Change in production!
          '';

        # Install script
        installScript = pkgs.writeShellScriptBin "install-llama-services" ''
          #!/usr/bin/env bash
          set -euo pipefail

          echo "Installing local-llama services..."

          # Create directories
          sudo mkdir -p /etc/llama-server
          sudo mkdir -p /etc/litellm

          # Copy configurations
          echo "Copying model configurations..."
          for conf in configs/*.conf; do
            sudo cp "$conf" /etc/llama-server/
          done

          # Copy systemd services
          echo "Installing systemd services..."
          sudo cp systemd/llama-server@.service /etc/systemd/system/
          sudo cp systemd/llama-nginx-proxy.service /etc/systemd/system/

          # Reload systemd
          sudo systemctl daemon-reload

          echo "Enabling services..."
          sudo systemctl enable llama-server@qwen3-coder
          sudo systemctl enable llama-server@qwen3-embed
          sudo systemctl enable llama-server@bge-reranker
          sudo systemctl enable llama-nginx-proxy

          echo ""
          echo "Installation complete!"
          echo "Start services with: sudo systemctl start llama-server@qwen3-coder"
        '';

        # Firewall update script
        firewallScript = pkgs.writeShellScriptBin "llama-firewall" ''
          #!/usr/bin/env bash
          # Firewall rule update script for local-llama infrastructure
          set -euo pipefail

          # Configuration
          LITELLM_PORT="${"$"}{LITELLM_PORT:-${toString defaultConfig.proxy.port}}"
          CHAT_PORT="${"$"}{CHAT_PORT:-${toString defaultConfig.models.chat.port}}"
          EMBED_PORT="${"$"}{EMBED_PORT:-${toString defaultConfig.models.embed.port}}"
          RERANK_PORT="${"$"}{RERANK_PORT:-${toString defaultConfig.models.rerank.port}}"

          PORTS=("$LITELLM_PORT" "$CHAT_PORT" "$EMBED_PORT" "$RERANK_PORT")

          declare -A PORT_NAMES=(
              ["$LITELLM_PORT"]="LiteLLM-Proxy"
              ["$CHAT_PORT"]="LLaMA-Chat"
              ["$EMBED_PORT"]="LLaMA-Embed"
              ["$RERANK_PORT"]="LLaMA-Rerank"
          )

          RED='\033[0;31m'
          GREEN='\033[0;32m'
          YELLOW='\033[1;33m'
          BLUE='\033[0;34m'
          NC='\033[0m'

          log_info() { echo -e "${"$"}{BLUE}[INFO]${"$"}{NC} $1"; }
          log_success() { echo -e "${"$"}{GREEN}[OK]${"$"}{NC} $1"; }
          log_warn() { echo -e "${"$"}{YELLOW}[WARN]${"$"}{NC} $1"; }
          log_error() { echo -e "${"$"}{RED}[ERROR]${"$"}{NC} $1"; }

          detect_firewall() {
              if command -v ufw &>/dev/null && sudo ufw status 2>/dev/null | grep -q "Status: active"; then
                  echo "ufw"
              elif command -v firewall-cmd &>/dev/null && sudo systemctl is-active firewalld &>/dev/null; then
                  echo "firewalld"
              elif command -v iptables &>/dev/null; then
                  echo "iptables"
              else
                  echo "none"
              fi
          }

          ufw_enable_ports() {
              log_info "Configuring UFW firewall rules..."
              for port in "${"$"}{PORTS[@]}"; do
                  local name="${"$"}{PORT_NAMES[$port]}"
                  if sudo ufw status | grep -q "$port/tcp.*ALLOW"; then
                      log_success "Port $port ($name) already allowed"
                  else
                      sudo ufw allow "$port/tcp" comment "$name"
                      log_success "Opened port $port ($name)"
                  fi
              done
              sudo ufw reload
          }

          ufw_disable_ports() {
              log_info "Removing UFW firewall rules..."
              for port in "${"$"}{PORTS[@]}"; do
                  local name="${"$"}{PORT_NAMES[$port]}"
                  if sudo ufw status | grep -q "$port/tcp"; then
                      sudo ufw delete allow "$port/tcp"
                      log_success "Closed port $port ($name)"
                  else
                      log_warn "Port $port ($name) was not open"
                  fi
              done
          }

          ufw_status() {
              echo ""
              log_info "UFW Firewall Status:"
              echo ""
              sudo ufw status verbose | head -20
              echo ""
              log_info "LLM Service Ports:"
              for port in "${"$"}{PORTS[@]}"; do
                  local name="${"$"}{PORT_NAMES[$port]}"
                  if sudo ufw status | grep -q "$port/tcp.*ALLOW"; then
                      echo -e "  ${"$"}{GREEN}[OPEN]${"$"}{NC}   $port/tcp - $name"
                  else
                      echo -e "  ${"$"}{RED}[CLOSED]${"$"}{NC} $port/tcp - $name"
                  fi
              done
          }

          iptables_enable_ports() {
              log_info "Configuring iptables rules..."
              for port in "${"$"}{PORTS[@]}"; do
                  local name="${"$"}{PORT_NAMES[$port]}"
                  if sudo iptables -C INPUT -p tcp --dport "$port" -j ACCEPT 2>/dev/null; then
                      log_success "Port $port ($name) already allowed"
                  else
                      sudo iptables -A INPUT -p tcp --dport "$port" -j ACCEPT -m comment --comment "$name"
                      log_success "Opened port $port ($name)"
                  fi
              done
          }

          iptables_disable_ports() {
              log_info "Removing iptables rules..."
              for port in "${"$"}{PORTS[@]}"; do
                  local name="${"$"}{PORT_NAMES[$port]}"
                  sudo iptables -D INPUT -p tcp --dport "$port" -j ACCEPT 2>/dev/null && \
                      log_success "Closed port $port ($name)" || \
                      log_warn "Port $port ($name) was not open"
              done
          }

          iptables_status() {
              echo ""
              log_info "iptables INPUT Chain:"
              echo ""
              sudo iptables -L INPUT -n --line-numbers | head -20
              echo ""
              log_info "LLM Service Ports:"
              for port in "${"$"}{PORTS[@]}"; do
                  local name="${"$"}{PORT_NAMES[$port]}"
                  if sudo iptables -C INPUT -p tcp --dport "$port" -j ACCEPT 2>/dev/null; then
                      echo -e "  ${"$"}{GREEN}[OPEN]${"$"}{NC}   $port/tcp - $name"
                  else
                      echo -e "  ${"$"}{RED}[CLOSED]${"$"}{NC} $port/tcp - $name"
                  fi
              done
          }

          main() {
              local action="${"$"}{1:-status}"
              local firewall
              firewall=$(detect_firewall)

              if [[ "$firewall" == "none" ]]; then
                  log_warn "No active firewall detected. Ports should be accessible."
                  exit 0
              fi

              log_info "Detected firewall: $firewall"

              case "$action" in
                  enable|open|allow)
                      case "$firewall" in
                          ufw) ufw_enable_ports ;;
                          iptables) iptables_enable_ports ;;
                      esac
                      echo ""
                      log_success "Firewall rules updated. LLM services accessible from network."
                      ;;
                  disable|close|deny)
                      case "$firewall" in
                          ufw) ufw_disable_ports ;;
                          iptables) iptables_disable_ports ;;
                      esac
                      echo ""
                      log_success "Firewall rules removed. LLM services only accessible locally."
                      ;;
                  status|show)
                      case "$firewall" in
                          ufw) ufw_status ;;
                          iptables) iptables_status ;;
                      esac
                      ;;
                  *)
                      echo "Usage: $0 [enable|disable|status]"
                      echo ""
                      echo "Commands:"
                      echo "  enable   Open ports for LLM services (allows network access)"
                      echo "  disable  Close ports for LLM services (localhost only)"
                      echo "  status   Show current firewall rules for LLM ports"
                      echo ""
                      echo "Ports managed:"
                      for port in "${"$"}{PORTS[@]}"; do
                          echo "  $port - ${"$"}{PORT_NAMES[$port]}"
                      done
                      exit 1
                      ;;
              esac
          }

          main "$@"
        '';

        # LiteLLM start script
        litellmScript = pkgs.writeShellScriptBin "start-litellm-proxy" ''
          #!/usr/bin/env bash
          set -euo pipefail

          CONFIG_FILE="''${1:-configs/litellm-config.yaml}"
          PORT="''${2:-4000}"

          echo "Starting LiteLLM proxy on port $PORT..."
          echo "Config: $CONFIG_FILE"
          echo ""
          echo "Endpoints available:"
          echo "  - Chat:       http://localhost:$PORT/v1/chat/completions"
          echo "  - Embeddings: http://localhost:$PORT/v1/embeddings"
          echo "  - Rerank:     http://localhost:$PORT/v1/rerank"
          echo "  - Models:     http://localhost:$PORT/v1/models"
          echo ""

          exec litellm --config "$CONFIG_FILE" --port "$PORT"
        '';

        # Open WebUI deployment script
        webuiScript = pkgs.writeShellScriptBin "llama-webui" ''
          #!/usr/bin/env bash
          set -euo pipefail

          CONTAINER_NAME="open-webui"
          IMAGE="ghcr.io/open-webui/open-webui:main"
          WEBUI_PORT="${"$"}{WEBUI_PORT:-3000}"
          LITELLM_URL="${"$"}{LITELLM_URL:-http://localhost:${toString defaultConfig.proxy.port}/v1}"
          LITELLM_KEY="${"$"}{LITELLM_KEY:-sk-local-llm-master}"

          RED='\033[0;31m'
          GREEN='\033[0;32m'
          YELLOW='\033[1;33m'
          BLUE='\033[0;34m'
          NC='\033[0m'

          log_info() { echo -e "${"$"}{BLUE}[INFO]${"$"}{NC} $1"; }
          log_success() { echo -e "${"$"}{GREEN}[OK]${"$"}{NC} $1"; }
          log_warn() { echo -e "${"$"}{YELLOW}[WARN]${"$"}{NC} $1"; }

          check_docker() {
              if ! command -v docker &>/dev/null; then
                  echo "Docker is not installed"
                  exit 1
              fi
          }

          start_webui() {
              check_docker
              if docker ps --format '{{.Names}}' | grep -q "^${"$"}{CONTAINER_NAME}$"; then
                  log_warn "Open WebUI is already running"
                  echo "URL: http://localhost:${"$"}{WEBUI_PORT}"
                  return 0
              fi
              docker rm "${"$"}{CONTAINER_NAME}" 2>/dev/null || true
              log_info "Starting Open WebUI on port ${"$"}{WEBUI_PORT}..."
              docker run -d --name "${"$"}{CONTAINER_NAME}" \
                  --network host \
                  -v open-webui-data:/app/backend/data \
                  -e OPENAI_API_BASE_URL="${"$"}{LITELLM_URL}" \
                  -e OPENAI_API_KEY="${"$"}{LITELLM_KEY}" \
                  -e WEBUI_AUTH=false \
                  -e PORT="${"$"}{WEBUI_PORT}" \
                  --restart unless-stopped \
                  "${"$"}{IMAGE}" >/dev/null
              log_info "Waiting for startup..."
              sleep 8
              log_success "Open WebUI running at http://localhost:${"$"}{WEBUI_PORT}"
          }

          stop_webui() {
              check_docker
              if docker ps --format '{{.Names}}' | grep -q "^${"$"}{CONTAINER_NAME}$"; then
                  log_info "Stopping Open WebUI..."
                  docker stop "${"$"}{CONTAINER_NAME}" >/dev/null
                  docker rm "${"$"}{CONTAINER_NAME}" >/dev/null
                  log_success "Stopped"
              else
                  log_warn "Open WebUI is not running"
              fi
          }

          show_status() {
              check_docker
              if docker ps --format '{{.Names}}' | grep -q "^${"$"}{CONTAINER_NAME}$"; then
                  echo -e "Status: ${"$"}{GREEN}Running${"$"}{NC}"
                  echo "URL: http://localhost:${"$"}{WEBUI_PORT}"
              else
                  echo -e "Status: ${"$"}{RED}Stopped${"$"}{NC}"
              fi
          }

          case "${"$"}{1:-start}" in
              start) start_webui ;;
              stop) stop_webui ;;
              status) show_status ;;
              logs) docker logs -f "${"$"}{CONTAINER_NAME}" ;;
              *) echo "Usage: $0 [start|stop|status|logs]"; exit 1 ;;
          esac
        '';

      in
      {
        # Development shell
        devShells.default = pkgs.mkShell {
          name = "local-llama-dev";

          buildInputs = with pkgs; [
            # Python for LiteLLM
            (python311.withPackages (
              ps: with ps; [
                litellm
                redis
                uvicorn
                fastapi
              ]
            ))

            # Tools
            jq
            curl
            httpie

            # Nix tools
            nil # Nix LSP
            nixpkgs-fmt
          ];

          shellHook = ''
            echo "Local LLama Development Environment"
            echo ""
            echo "Available commands:"
            echo "  nix run .#install   - Install systemd services"
            echo "  nix run .#litellm   - Start LiteLLM AI proxy"
            echo "  nix run .#firewall  - Manage firewall rules (enable/disable/status)"
            echo "  nix run .#webui     - Deploy Open WebUI chat interface (start/stop/status)"
            echo ""
            echo "Scripts:"
            echo "  ./scripts/firewall-update.sh [enable|disable|status]"
            echo "  ./scripts/deploy-webui.sh [start|stop|status|logs|update]"
            echo ""
            echo "ROCm environment configured for ${defaultConfig.rocm.gpuTarget}"

            ${builtins.concatStringsSep "\n" (
              builtins.attrValues (builtins.mapAttrs (k: v: "export ${k}=\"${v}\"") rocmEnv)
            )}
          '';
        };

        # Packages
        packages = {
          install-script = installScript;
          litellm-script = litellmScript;
          firewall-script = firewallScript;
          webui-script = webuiScript;
          litellm-config = makeLiteLLMConfig defaultConfig;

          default = installScript;
        };

        # Apps
        apps = {
          install = {
            type = "app";
            program = "${installScript}/bin/install-llama-services";
          };

          litellm = {
            type = "app";
            program = "${litellmScript}/bin/start-litellm-proxy";
          };

          firewall = {
            type = "app";
            program = "${firewallScript}/bin/llama-firewall";
          };

          webui = {
            type = "app";
            program = "${webuiScript}/bin/llama-webui";
          };

          default = self.apps.${system}.install;
        };
      }
    )
    // {
      # NixOS module (system-wide)
      nixosModules.default =
        {
          config,
          lib,
          pkgs,
          ...
        }:
        with lib;
        let
          cfg = config.services.llama-server;
        in
        {
          options.services.llama-server = {
            enable = mkEnableOption "Local LLM server infrastructure";

            user = mkOption {
              type = types.str;
              default = "llama";
              description = "User to run llama-server as";
            };

            group = mkOption {
              type = types.str;
              default = "llama";
              description = "Group to run llama-server as";
            };

            modelsDir = mkOption {
              type = types.path;
              default = "/var/lib/llama/models";
              description = "Directory containing GGUF model files";
            };

            llamaCppPath = mkOption {
              type = types.path;
              default = "/opt/llama.cpp";
              description = "Path to llama.cpp installation";
            };

            rocm = {
              enable = mkEnableOption "ROCm/HIP acceleration";

              gpuTarget = mkOption {
                type = types.str;
                default = "gfx1151";
                description = "GPU architecture target";
              };

              umaEnabled = mkOption {
                type = types.bool;
                default = false;
                description = "Enable Unified Memory Architecture for APUs";
              };
            };

            models = mkOption {
              type = types.attrsOf (
                types.submodule {
                  options = {
                    modelFile = mkOption {
                      type = types.str;
                      description = "GGUF model filename";
                    };

                    port = mkOption {
                      type = types.port;
                      description = "Port to serve on";
                    };

                    contextSize = mkOption {
                      type = types.int;
                      default = 8192;
                      description = "Context window size";
                    };

                    parallelSlots = mkOption {
                      type = types.int;
                      default = 1;
                      description = "Number of parallel request slots";
                    };

                    extraFlags = mkOption {
                      type = types.listOf types.str;
                      default = [ ];
                      description = "Additional flags for llama-server";
                    };

                    mode = mkOption {
                      type = types.enum [
                        "chat"
                        "embedding"
                        "reranking"
                      ];
                      default = "chat";
                      description = "Server mode";
                    };
                  };
                }
              );
              default = { };
              description = "Model configurations";
            };

            proxy = {
              type = mkOption {
                type = types.enum [
                  "nginx"
                  "litellm"
                  "none"
                ];
                default = "litellm";
                description = "Proxy type to use";
              };

              port = mkOption {
                type = types.port;
                default = 4000;
                description = "Proxy port";
              };

              tls = {
                enable = mkEnableOption "TLS encryption";

                certFile = mkOption {
                  type = types.nullOr types.path;
                  default = null;
                  description = "Path to TLS certificate";
                };

                keyFile = mkOption {
                  type = types.nullOr types.path;
                  default = null;
                  description = "Path to TLS private key";
                };
              };

              cache = {
                enable = mkOption {
                  type = types.bool;
                  default = true;
                  description = "Enable response caching (recommended for embeddings)";
                };

                type = mkOption {
                  type = types.enum [
                    "local"
                    "redis"
                  ];
                  default = "local";
                  description = "Cache backend type";
                };

                ttl = mkOption {
                  type = types.int;
                  default = 3600;
                  description = "Cache TTL in seconds";
                };
              };
            };
          };

          config = mkIf cfg.enable {
            # Implementation would go here for full NixOS integration
            # This is a template for the module structure
            warnings = [
              "llama-server NixOS module is a template - see flake for full implementation"
            ];
          };
        };

      # Home-manager module (user-level)
      homeManagerModules.default =
        {
          config,
          lib,
          pkgs,
          ...
        }:
        with lib;
        let
          cfg = config.services.llama-server;
        in
        {
          options.services.llama-server = {
            enable = mkEnableOption "Local LLM server (user services)";
            # Similar options as NixOS module but for user systemd
          };

          config = mkIf cfg.enable {
            warnings = [
              "llama-server home-manager module is a template - see flake for full implementation"
            ];
          };
        };
    };
}
