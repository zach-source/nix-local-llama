{
  description = "Local LLM infrastructure for AMD APU/GPU with ROCm acceleration";

  inputs = {
    nixpkgs.url = "github:NixOS/nixpkgs/nixos-unstable";
    flake-utils.url = "github:numtide/flake-utils";
  };

  outputs =
    {
      self,
      nixpkgs,
      flake-utils,
    }:
    let
      # Supported systems (primarily Linux with ROCm)
      supportedSystems = [ "x86_64-linux" ];

      # Default paths - can be overridden via environment
      defaultPaths = {
        modelsDir = "$HOME/models";
        llamaCppDir = "$HOME/llama.cpp";
        rocmPath = "/opt/rocm";
        configDir = "/etc/llama-server";
      };

      # Default configuration
      defaultConfig = {
        rocm = {
          gpuTarget = "gfx1151"; # Strix Halo APU
          version = "7.1.1";
          umaEnabled = true; # Unified Memory for APUs
        };

        models = {
          chat = {
            name = "qwen3-coder";
            port = 8000;
            contextSize = 262144;
            parallelSlots = 1;
          };
          embed = {
            name = "qwen3-embed";
            port = 8001;
            contextSize = 8192;
            parallelSlots = 4;
          };
          rerank = {
            name = "bge-reranker";
            port = 8002;
            contextSize = 512;
            parallelSlots = 4;
          };
        };

        proxy = {
          type = "litellm"; # "nginx" or "litellm"
          port = 4000; # LiteLLM default port
          tlsPort = 8443;
        };

        # Envoy gateway configuration
        envoy = {
          port = 4001; # Unified gateway port
          adminPort = 9901;
        };
      };
    in
    flake-utils.lib.eachSystem supportedSystems (
      system:
      let
        pkgs = import nixpkgs {
          inherit system;
          config.allowUnfree = true;
        };

        # Import the unified LLM configuration module with all services enabled
        llmConfig = import ./nix/llm-config.nix {
          inherit pkgs;
          # Paths read from environment or use defaults
          modelsDir = builtins.getEnv "MODELS_DIR";
          llamaCppDir = builtins.getEnv "LLAMA_DIR";
          rocmPath = "/opt/rocm";
          configDir = "/etc/llama-server";
          # All services enabled by default
          enableChat = true;
          enableEmbedding = true;
          enableReranking = true;
          # Envoy enabled, AIGW optional
          enableEnvoy = true;
          enableAigw = false;
          enableLitellm = false;
        };

        # ROCm environment from llmConfig
        rocmEnv = llmConfig.activeConfig.hardware.environment;

        # Generate systemd service for a model
        makeModelService =
          name: cfg:
          pkgs.writeText "llama-server-${name}.service" ''
            [Unit]
            Description=LLaMA Server (${name})
            Documentation=https://github.com/ggerganov/llama.cpp
            After=network.target
            Wants=network-online.target

            [Service]
            Type=simple
            User=%u
            Group=%u

            ${builtins.concatStringsSep "\n" (
              builtins.attrValues (builtins.mapAttrs (k: v: "Environment=\"${k}=${v}\"") rocmEnv)
            )}

            EnvironmentFile=/etc/llama-server/${name}.conf
            Environment="EXTRA_FLAGS="

            ExecStart=/bin/bash -c "''${LLAMA_BIN} --model ''${MODEL_PATH} --host ''${HOST} --port ''${PORT} --ctx-size ''${CTX_SIZE} --n-gpu-layers ''${N_GPU_LAYERS} ''${EXTRA_FLAGS}"

            Restart=on-failure
            RestartSec=10
            StartLimitBurst=3
            StartLimitIntervalSec=60

            LimitNOFILE=65536
            LimitMEMLOCK=infinity

            StandardOutput=journal
            StandardError=journal
            SyslogIdentifier=llama-server-${name}

            [Install]
            WantedBy=multi-user.target
          '';

        # LiteLLM configuration generator
        makeLiteLLMConfig =
          cfg:
          pkgs.writeText "litellm-config.yaml" ''
            # LiteLLM Proxy Configuration
            # Generated by local-llama flake

            model_list:
              # Chat/Completions model - aliased as common OpenAI models
              - model_name: gpt-4
                litellm_params:
                  model: openai/${cfg.models.chat.name}
                  api_base: http://localhost:${toString cfg.models.chat.port}/v1
                  api_key: "sk-local"

              - model_name: gpt-4-turbo
                litellm_params:
                  model: openai/${cfg.models.chat.name}
                  api_base: http://localhost:${toString cfg.models.chat.port}/v1
                  api_key: "sk-local"

              - model_name: gpt-3.5-turbo
                litellm_params:
                  model: openai/${cfg.models.chat.name}
                  api_base: http://localhost:${toString cfg.models.chat.port}/v1
                  api_key: "sk-local"

              - model_name: ${cfg.models.chat.name}
                litellm_params:
                  model: openai/${cfg.models.chat.name}
                  api_base: http://localhost:${toString cfg.models.chat.port}/v1
                  api_key: "sk-local"

              # Embeddings model
              - model_name: text-embedding-3-small
                litellm_params:
                  model: openai/${cfg.models.embed.name}
                  api_base: http://localhost:${toString cfg.models.embed.port}/v1
                  api_key: "sk-local"

              - model_name: text-embedding-3-large
                litellm_params:
                  model: openai/${cfg.models.embed.name}
                  api_base: http://localhost:${toString cfg.models.embed.port}/v1
                  api_key: "sk-local"

              - model_name: text-embedding-ada-002
                litellm_params:
                  model: openai/${cfg.models.embed.name}
                  api_base: http://localhost:${toString cfg.models.embed.port}/v1
                  api_key: "sk-local"

              - model_name: ${cfg.models.embed.name}
                litellm_params:
                  model: openai/${cfg.models.embed.name}
                  api_base: http://localhost:${toString cfg.models.embed.port}/v1
                  api_key: "sk-local"

              # Reranker model
              - model_name: rerank-english-v3.0
                litellm_params:
                  model: openai/${cfg.models.rerank.name}
                  api_base: http://localhost:${toString cfg.models.rerank.port}/v1
                  api_key: "sk-local"

              - model_name: ${cfg.models.rerank.name}
                litellm_params:
                  model: openai/${cfg.models.rerank.name}
                  api_base: http://localhost:${toString cfg.models.rerank.port}/v1
                  api_key: "sk-local"

            litellm_settings:
              # Enable caching (critical for embeddings)
              cache: true
              cache_params:
                type: "local"  # Use "redis" for persistent cache
                ttl: 3600      # Cache TTL in seconds

              # Request settings
              request_timeout: 600

              # Telemetry
              success_callback: []
              failure_callback: []

            general_settings:
              master_key: "sk-local-llm-master"  # Change in production!
          '';

        # Install script
        installScript = pkgs.writeShellScriptBin "install-llama-services" ''
          #!/usr/bin/env bash
          set -euo pipefail

          echo "Installing local-llama services..."

          # Create directories
          sudo mkdir -p /etc/llama-server
          sudo mkdir -p /etc/litellm

          # Copy configurations
          echo "Copying model configurations..."
          for conf in configs/*.conf; do
            sudo cp "$conf" /etc/llama-server/
          done

          # Copy systemd services
          echo "Installing systemd services..."
          sudo cp systemd/llama-server@.service /etc/systemd/system/
          sudo cp systemd/llama-nginx-proxy.service /etc/systemd/system/

          # Reload systemd
          sudo systemctl daemon-reload

          echo "Enabling services..."
          sudo systemctl enable llama-server@qwen3-coder
          sudo systemctl enable llama-server@qwen3-embed
          sudo systemctl enable llama-server@bge-reranker
          sudo systemctl enable llama-nginx-proxy

          echo ""
          echo "Installation complete!"
          echo "Start services with: sudo systemctl start llama-server@qwen3-coder"
        '';

        # Firewall update script
        firewallScript = pkgs.writeShellScriptBin "llama-firewall" ''
          #!/usr/bin/env bash
          # Firewall rule update script for local-llama infrastructure
          set -euo pipefail

          # Configuration
          ENVOY_PORT="${"$"}{ENVOY_PORT:-${toString defaultConfig.envoy.port}}"
          LITELLM_PORT="${"$"}{LITELLM_PORT:-${toString defaultConfig.proxy.port}}"
          CHAT_PORT="${"$"}{CHAT_PORT:-${toString defaultConfig.models.chat.port}}"
          EMBED_PORT="${"$"}{EMBED_PORT:-${toString defaultConfig.models.embed.port}}"
          RERANK_PORT="${"$"}{RERANK_PORT:-${toString defaultConfig.models.rerank.port}}"

          PORTS=("$ENVOY_PORT" "$LITELLM_PORT" "$CHAT_PORT" "$EMBED_PORT" "$RERANK_PORT")

          declare -A PORT_NAMES=(
              ["$ENVOY_PORT"]="Envoy-Gateway"
              ["$LITELLM_PORT"]="LiteLLM-Proxy"
              ["$CHAT_PORT"]="LLaMA-Chat"
              ["$EMBED_PORT"]="LLaMA-Embed"
              ["$RERANK_PORT"]="LLaMA-Rerank"
          )

          RED='\033[0;31m'
          GREEN='\033[0;32m'
          YELLOW='\033[1;33m'
          BLUE='\033[0;34m'
          NC='\033[0m'

          log_info() { echo -e "${"$"}{BLUE}[INFO]${"$"}{NC} $1"; }
          log_success() { echo -e "${"$"}{GREEN}[OK]${"$"}{NC} $1"; }
          log_warn() { echo -e "${"$"}{YELLOW}[WARN]${"$"}{NC} $1"; }
          log_error() { echo -e "${"$"}{RED}[ERROR]${"$"}{NC} $1"; }

          detect_firewall() {
              if command -v ufw &>/dev/null && sudo ufw status 2>/dev/null | grep -q "Status: active"; then
                  echo "ufw"
              elif command -v firewall-cmd &>/dev/null && sudo systemctl is-active firewalld &>/dev/null; then
                  echo "firewalld"
              elif command -v iptables &>/dev/null; then
                  echo "iptables"
              else
                  echo "none"
              fi
          }

          ufw_enable_ports() {
              log_info "Configuring UFW firewall rules..."
              for port in "${"$"}{PORTS[@]}"; do
                  local name="${"$"}{PORT_NAMES[$port]}"
                  if sudo ufw status | grep -q "$port/tcp.*ALLOW"; then
                      log_success "Port $port ($name) already allowed"
                  else
                      sudo ufw allow "$port/tcp" comment "$name"
                      log_success "Opened port $port ($name)"
                  fi
              done
              sudo ufw reload
          }

          ufw_disable_ports() {
              log_info "Removing UFW firewall rules..."
              for port in "${"$"}{PORTS[@]}"; do
                  local name="${"$"}{PORT_NAMES[$port]}"
                  if sudo ufw status | grep -q "$port/tcp"; then
                      sudo ufw delete allow "$port/tcp"
                      log_success "Closed port $port ($name)"
                  else
                      log_warn "Port $port ($name) was not open"
                  fi
              done
          }

          ufw_status() {
              echo ""
              log_info "UFW Firewall Status:"
              echo ""
              sudo ufw status verbose | head -20
              echo ""
              log_info "LLM Service Ports:"
              for port in "${"$"}{PORTS[@]}"; do
                  local name="${"$"}{PORT_NAMES[$port]}"
                  if sudo ufw status | grep -q "$port/tcp.*ALLOW"; then
                      echo -e "  ${"$"}{GREEN}[OPEN]${"$"}{NC}   $port/tcp - $name"
                  else
                      echo -e "  ${"$"}{RED}[CLOSED]${"$"}{NC} $port/tcp - $name"
                  fi
              done
          }

          iptables_enable_ports() {
              log_info "Configuring iptables rules..."
              for port in "${"$"}{PORTS[@]}"; do
                  local name="${"$"}{PORT_NAMES[$port]}"
                  if sudo iptables -C INPUT -p tcp --dport "$port" -j ACCEPT 2>/dev/null; then
                      log_success "Port $port ($name) already allowed"
                  else
                      sudo iptables -A INPUT -p tcp --dport "$port" -j ACCEPT -m comment --comment "$name"
                      log_success "Opened port $port ($name)"
                  fi
              done
          }

          iptables_disable_ports() {
              log_info "Removing iptables rules..."
              for port in "${"$"}{PORTS[@]}"; do
                  local name="${"$"}{PORT_NAMES[$port]}"
                  sudo iptables -D INPUT -p tcp --dport "$port" -j ACCEPT 2>/dev/null && \
                      log_success "Closed port $port ($name)" || \
                      log_warn "Port $port ($name) was not open"
              done
          }

          iptables_status() {
              echo ""
              log_info "iptables INPUT Chain:"
              echo ""
              sudo iptables -L INPUT -n --line-numbers | head -20
              echo ""
              log_info "LLM Service Ports:"
              for port in "${"$"}{PORTS[@]}"; do
                  local name="${"$"}{PORT_NAMES[$port]}"
                  if sudo iptables -C INPUT -p tcp --dport "$port" -j ACCEPT 2>/dev/null; then
                      echo -e "  ${"$"}{GREEN}[OPEN]${"$"}{NC}   $port/tcp - $name"
                  else
                      echo -e "  ${"$"}{RED}[CLOSED]${"$"}{NC} $port/tcp - $name"
                  fi
              done
          }

          main() {
              local action="${"$"}{1:-status}"
              local firewall
              firewall=$(detect_firewall)

              if [[ "$firewall" == "none" ]]; then
                  log_warn "No active firewall detected. Ports should be accessible."
                  exit 0
              fi

              log_info "Detected firewall: $firewall"

              case "$action" in
                  enable|open|allow)
                      case "$firewall" in
                          ufw) ufw_enable_ports ;;
                          iptables) iptables_enable_ports ;;
                      esac
                      echo ""
                      log_success "Firewall rules updated. LLM services accessible from network."
                      ;;
                  disable|close|deny)
                      case "$firewall" in
                          ufw) ufw_disable_ports ;;
                          iptables) iptables_disable_ports ;;
                      esac
                      echo ""
                      log_success "Firewall rules removed. LLM services only accessible locally."
                      ;;
                  status|show)
                      case "$firewall" in
                          ufw) ufw_status ;;
                          iptables) iptables_status ;;
                      esac
                      ;;
                  *)
                      echo "Usage: $0 [enable|disable|status]"
                      echo ""
                      echo "Commands:"
                      echo "  enable   Open ports for LLM services (allows network access)"
                      echo "  disable  Close ports for LLM services (localhost only)"
                      echo "  status   Show current firewall rules for LLM ports"
                      echo ""
                      echo "Ports managed:"
                      for port in "${"$"}{PORTS[@]}"; do
                          echo "  $port - ${"$"}{PORT_NAMES[$port]}"
                      done
                      exit 1
                      ;;
              esac
          }

          main "$@"
        '';

        # LiteLLM start script
        litellmScript = pkgs.writeShellScriptBin "start-litellm-proxy" ''
          #!/usr/bin/env bash
          set -euo pipefail

          CONFIG_FILE="''${1:-configs/litellm-config.yaml}"
          PORT="''${2:-4000}"

          echo "Starting LiteLLM proxy on port $PORT..."
          echo "Config: $CONFIG_FILE"
          echo ""
          echo "Endpoints available:"
          echo "  - Chat:       http://localhost:$PORT/v1/chat/completions"
          echo "  - Embeddings: http://localhost:$PORT/v1/embeddings"
          echo "  - Rerank:     http://localhost:$PORT/v1/rerank"
          echo "  - Models:     http://localhost:$PORT/v1/models"
          echo ""

          exec litellm --config "$CONFIG_FILE" --port "$PORT"
        '';

        # Open WebUI deployment script
        webuiScript = pkgs.writeShellScriptBin "llama-webui" ''
          #!/usr/bin/env bash
          set -euo pipefail

          CONTAINER_NAME="open-webui"
          IMAGE="ghcr.io/open-webui/open-webui:main"
          WEBUI_PORT="${"$"}{WEBUI_PORT:-3000}"
          LITELLM_URL="${"$"}{LITELLM_URL:-http://localhost:${toString defaultConfig.proxy.port}/v1}"
          LITELLM_KEY="${"$"}{LITELLM_KEY:-sk-local-llm-master}"

          RED='\033[0;31m'
          GREEN='\033[0;32m'
          YELLOW='\033[1;33m'
          BLUE='\033[0;34m'
          NC='\033[0m'

          log_info() { echo -e "${"$"}{BLUE}[INFO]${"$"}{NC} $1"; }
          log_success() { echo -e "${"$"}{GREEN}[OK]${"$"}{NC} $1"; }
          log_warn() { echo -e "${"$"}{YELLOW}[WARN]${"$"}{NC} $1"; }

          check_docker() {
              if ! command -v docker &>/dev/null; then
                  echo "Docker is not installed"
                  exit 1
              fi
          }

          start_webui() {
              check_docker
              if docker ps --format '{{.Names}}' | grep -q "^${"$"}{CONTAINER_NAME}$"; then
                  log_warn "Open WebUI is already running"
                  echo "URL: http://localhost:${"$"}{WEBUI_PORT}"
                  return 0
              fi
              docker rm "${"$"}{CONTAINER_NAME}" 2>/dev/null || true
              log_info "Starting Open WebUI on port ${"$"}{WEBUI_PORT}..."
              docker run -d --name "${"$"}{CONTAINER_NAME}" \
                  --network host \
                  -v open-webui-data:/app/backend/data \
                  -e OPENAI_API_BASE_URL="${"$"}{LITELLM_URL}" \
                  -e OPENAI_API_KEY="${"$"}{LITELLM_KEY}" \
                  -e WEBUI_AUTH=false \
                  -e PORT="${"$"}{WEBUI_PORT}" \
                  --restart unless-stopped \
                  "${"$"}{IMAGE}" >/dev/null
              log_info "Waiting for startup..."
              sleep 8
              log_success "Open WebUI running at http://localhost:${"$"}{WEBUI_PORT}"
          }

          stop_webui() {
              check_docker
              if docker ps --format '{{.Names}}' | grep -q "^${"$"}{CONTAINER_NAME}$"; then
                  log_info "Stopping Open WebUI..."
                  docker stop "${"$"}{CONTAINER_NAME}" >/dev/null
                  docker rm "${"$"}{CONTAINER_NAME}" >/dev/null
                  log_success "Stopped"
              else
                  log_warn "Open WebUI is not running"
              fi
          }

          show_status() {
              check_docker
              if docker ps --format '{{.Names}}' | grep -q "^${"$"}{CONTAINER_NAME}$"; then
                  echo -e "Status: ${"$"}{GREEN}Running${"$"}{NC}"
                  echo "URL: http://localhost:${"$"}{WEBUI_PORT}"
              else
                  echo -e "Status: ${"$"}{RED}Stopped${"$"}{NC}"
              fi
          }

          case "${"$"}{1:-start}" in
              start) start_webui ;;
              stop) stop_webui ;;
              status) show_status ;;
              logs) docker logs -f "${"$"}{CONTAINER_NAME}" ;;
              *) echo "Usage: $0 [start|stop|status|logs]"; exit 1 ;;
          esac
        '';

        # llama.cpp build helper script
        buildLlamaScript = pkgs.writeShellScriptBin "llama-build" ''
          #!/usr/bin/env bash
          set -euo pipefail

          LLAMA_DIR="${"$"}{LLAMA_DIR:-$HOME/llama.cpp}"
          BUILD_TYPE="${"$"}{1:-rocm}"
          GPU_TARGET="${"$"}{GPU_TARGET:-gfx1151}"
          ROCM_PATH="${"$"}{ROCM_PATH:-/opt/rocm}"

          RED='\033[0;31m'
          GREEN='\033[0;32m'
          YELLOW='\033[1;33m'
          BLUE='\033[0;34m'
          NC='\033[0m'

          log_info() { echo -e "${"$"}{BLUE}[INFO]${"$"}{NC} $1"; }
          log_success() { echo -e "${"$"}{GREEN}[OK]${"$"}{NC} $1"; }
          log_warn() { echo -e "${"$"}{YELLOW}[WARN]${"$"}{NC} $1"; }
          log_error() { echo -e "${"$"}{RED}[ERROR]${"$"}{NC} $1"; }

          clone_or_update() {
              if [[ -d "$LLAMA_DIR" ]]; then
                  log_info "Updating llama.cpp..."
                  cd "$LLAMA_DIR"
                  git fetch && git pull
              else
                  log_info "Cloning llama.cpp..."
                  git clone https://github.com/ggml-org/llama.cpp.git "$LLAMA_DIR"
                  cd "$LLAMA_DIR"
              fi
          }

          build_rocm() {
              local BUILD_DIR="$LLAMA_DIR/build-rocm"
              log_info "Building llama.cpp with ROCm for $GPU_TARGET..."

              # Check for rocWMMA
              local ROCWMMA_FLAG=""
              if [[ -f "$ROCM_PATH/include/rocwmma/rocwmma.hpp" ]]; then
                  log_info "rocWMMA found, enabling flash attention"
                  ROCWMMA_FLAG="-DGGML_HIP_ROCWMMA_FATTN=ON"
              fi

              rm -rf "$BUILD_DIR"
              mkdir -p "$BUILD_DIR"
              cd "$BUILD_DIR"

              export HIPCXX="$($ROCM_PATH/bin/hipconfig -l)/clang"
              export HIP_PATH="$($ROCM_PATH/bin/hipconfig -R)"

              cmake .. \
                  -DGGML_HIP=ON \
                  -DAMDGPU_TARGETS="$GPU_TARGET" \
                  -DCMAKE_BUILD_TYPE=Release \
                  -DCMAKE_PREFIX_PATH="$ROCM_PATH;/usr" \
                  -DGGML_NATIVE=OFF \
                  $ROCWMMA_FLAG

              cmake --build . --config Release -j$(nproc)

              log_success "ROCm build complete: $BUILD_DIR/bin/llama-server"
          }

          build_vulkan() {
              local BUILD_DIR="$LLAMA_DIR/build-vulkan"
              log_info "Building llama.cpp with Vulkan..."

              rm -rf "$BUILD_DIR"
              mkdir -p "$BUILD_DIR"
              cd "$BUILD_DIR"

              cmake .. \
                  -DGGML_VULKAN=ON \
                  -DCMAKE_BUILD_TYPE=Release

              cmake --build . --config Release -j$(nproc)

              log_success "Vulkan build complete: $BUILD_DIR/bin/llama-server"
          }

          build_cpu() {
              local BUILD_DIR="$LLAMA_DIR/build-cpu"
              log_info "Building llama.cpp (CPU only)..."

              rm -rf "$BUILD_DIR"
              mkdir -p "$BUILD_DIR"
              cd "$BUILD_DIR"

              cmake .. \
                  -DCMAKE_BUILD_TYPE=Release \
                  -DGGML_NATIVE=ON

              cmake --build . --config Release -j$(nproc)

              log_success "CPU build complete: $BUILD_DIR/bin/llama-server"
          }

          show_help() {
              echo "llama.cpp Build Helper"
              echo ""
              echo "Usage: $0 [build-type]"
              echo ""
              echo "Build types:"
              echo "  rocm     Build with ROCm/HIP for AMD GPUs (default)"
              echo "  vulkan   Build with Vulkan for cross-platform GPU"
              echo "  cpu      Build CPU-only version"
              echo "  update   Update llama.cpp source only"
              echo ""
              echo "Environment:"
              echo "  LLAMA_DIR    llama.cpp directory (default: ~/llama.cpp)"
              echo "  GPU_TARGET   GPU architecture (default: gfx1151)"
              echo "  ROCM_PATH    ROCm installation path (default: /opt/rocm)"
          }

          main() {
              case "$BUILD_TYPE" in
                  rocm|uma)
                      clone_or_update
                      build_rocm
                      ;;
                  vulkan)
                      clone_or_update
                      build_vulkan
                      ;;
                  cpu)
                      clone_or_update
                      build_cpu
                      ;;
                  update)
                      clone_or_update
                      log_success "Source updated"
                      ;;
                  help|--help|-h)
                      show_help
                      ;;
                  *)
                      log_error "Unknown build type: $BUILD_TYPE"
                      show_help
                      exit 1
                      ;;
              esac
          }

          main
        '';

        # Model download helper script
        downloadModelScript = pkgs.writeShellScriptBin "llama-download" ''
          #!/usr/bin/env bash
          set -euo pipefail

          MODELS_DIR="${"$"}{MODELS_DIR:-$HOME/models}"

          RED='\033[0;31m'
          GREEN='\033[0;32m'
          YELLOW='\033[1;33m'
          BLUE='\033[0;34m'
          NC='\033[0m'

          log_info() { echo -e "${"$"}{BLUE}[INFO]${"$"}{NC} $1"; }
          log_success() { echo -e "${"$"}{GREEN}[OK]${"$"}{NC} $1"; }
          log_warn() { echo -e "${"$"}{YELLOW}[WARN]${"$"}{NC} $1"; }

          # Model definitions - format: "repo:file" or "repo:file1,file2" for split models
          declare -A MODELS=(
              # Chat models (Devstral-2 123B is split into 2 files in Q4_K_M subdir)
              ["devstral2-123b"]="unsloth/Devstral-2-123B-Instruct-2512-GGUF:Q4_K_M/Devstral-2-123B-Instruct-2512-Q4_K_M-00001-of-00002.gguf,Q4_K_M/Devstral-2-123B-Instruct-2512-Q4_K_M-00002-of-00002.gguf"
              ["devstral2-24b"]="unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF:Devstral-Small-2-24B-Instruct-2512-Q8_0.gguf"
              ["qwen3-coder-30b"]="Qwen/Qwen3-Coder-30B-A3B-Instruct-GGUF:Qwen3-Coder-30B-A3B-Instruct-Q6_K.gguf"
              # Embedding models
              ["qwen3-embed-8b"]="Qwen/Qwen3-Embedding-8B-GGUF:Qwen3-Embedding-8B-Q8_0.gguf"
              ["nomic-embed"]="nomic-ai/nomic-embed-text-v1.5-GGUF:nomic-embed-text-v1.5.f16.gguf"
              # Reranking models (gpustack provides GGUF, not BAAI directly)
              ["bge-reranker"]="gpustack/bge-reranker-v2-m3-GGUF:bge-reranker-v2-m3-Q8_0.gguf"
          )

          download_model() {
              local key="$1"
              local spec="${"$"}{MODELS[$key]:-}"

              if [[ -z "$spec" ]]; then
                  log_warn "Unknown model: $key"
                  echo "Available models:"
                  for m in "${"$"}{!MODELS[@]}"; do
                      echo "  $m"
                  done
                  exit 1
              fi

              local repo="${"$"}{spec%%:*}"
              local files="${"$"}{spec#*:}"

              mkdir -p "$MODELS_DIR"

              # Handle multiple files (comma-separated)
              IFS=',' read -ra FILE_LIST <<< "$files"
              for file in "${"$"}{FILE_LIST[@]}"; do
                  local basename="${"$"}{file##*/}"
                  log_info "Downloading $basename from $repo..."

                  if command -v huggingface-cli &>/dev/null; then
                      huggingface-cli download "$repo" "$file" \
                          --local-dir "$MODELS_DIR" \
                          --local-dir-use-symlinks False
                  else
                      log_warn "huggingface-cli not found, using curl..."
                      curl -L -o "$MODELS_DIR/$basename" \
                          "https://huggingface.co/$repo/resolve/main/$file"
                  fi

                  log_success "Downloaded $basename"
              done
          }

          list_models() {
              echo "Available models:"
              echo ""
              echo "Chat Models:"
              echo "  devstral2-123b      Devstral-2-123B Q4_K_M (75GB, 2 files) - Best for agentic coding"
              echo "  devstral2-24b       Devstral-Small-2-24B Q8 (25GB) - Fast inference"
              echo "  qwen3-coder-30b     Qwen3-Coder-30B-A3B Q6 (24GB) - MoE coding model"
              echo ""
              echo "Embedding Models:"
              echo "  qwen3-embed-8b      Qwen3-Embedding-8B Q8 (8.5GB)"
              echo "  nomic-embed         Nomic-Embed-Text-v1.5 Q8 (0.5GB)"
              echo ""
              echo "Reranking Models:"
              echo "  bge-reranker        BGE-Reranker-v2-m3 Q8 (1.2GB)"
          }

          list_installed() {
              log_info "Installed models in $MODELS_DIR:"
              if [[ -d "$MODELS_DIR" ]]; then
                  ls -lh "$MODELS_DIR"/*.gguf 2>/dev/null || echo "  No GGUF models found"
              else
                  echo "  Models directory not found"
              fi
          }

          download_stack() {
              log_info "Downloading recommended model stack..."
              download_model "devstral2-123b"
              download_model "qwen3-embed-8b"
              download_model "bge-reranker"
              log_success "All models downloaded!"
          }

          case "${"$"}{1:-list}" in
              list)
                  list_models
                  ;;
              installed|ls)
                  list_installed
                  ;;
              stack|all)
                  download_stack
                  ;;
              *)
                  download_model "$1"
                  ;;
          esac
        '';

        # Generate Envoy configuration
        makeEnvoyConfig =
          cfg:
          pkgs.writeText "envoy.yaml" ''
            # Envoy Proxy Configuration for Local LLM Infrastructure
            # Provides unified API gateway routing to multiple LLM backends
            #
            # Architecture:
            #   Port ${toString cfg.envoy.port} (Envoy) → Routes to:
            #     /v1/embeddings  → localhost:${toString cfg.models.embed.port} (${cfg.models.embed.name})
            #     /v1/rerank      → localhost:${toString cfg.models.rerank.port} (${cfg.models.rerank.name})
            #     /v1/chat/*      → localhost:${toString cfg.models.chat.port} (${cfg.models.chat.name})
            #     /v1/completions → localhost:${toString cfg.models.chat.port} (${cfg.models.chat.name})
            #     /health         → localhost:${toString cfg.models.chat.port} (health check)
            #
            # Start with: nix run .#envoy

            admin:
              address:
                socket_address:
                  address: 127.0.0.1
                  port_value: ${toString cfg.envoy.adminPort}

            static_resources:
              listeners:
              - name: llm_gateway
                address:
                  socket_address:
                    address: 0.0.0.0
                    port_value: ${toString cfg.envoy.port}
                filter_chains:
                - filters:
                  - name: envoy.filters.network.http_connection_manager
                    typed_config:
                      "@type": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager
                      stat_prefix: llm_gateway
                      codec_type: AUTO

                      # Increase timeouts for LLM inference (can be slow)
                      stream_idle_timeout: 600s
                      request_timeout: 600s

                      access_log:
                      - name: envoy.access_loggers.stdout
                        typed_config:
                          "@type": type.googleapis.com/envoy.extensions.access_loggers.stream.v3.StdoutAccessLog

                      http_filters:
                      - name: envoy.filters.http.router
                        typed_config:
                          "@type": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router

                      route_config:
                        name: llm_routes
                        virtual_hosts:
                        - name: llm_services
                          domains: ["*"]
                          routes:
                          # Embeddings → embed backend
                          - match:
                              prefix: "/v1/embeddings"
                            route:
                              cluster: llama_embed
                              timeout: 120s

                          # Reranking → rerank backend
                          - match:
                              prefix: "/v1/rerank"
                            route:
                              cluster: llama_rerank
                              timeout: 60s

                          # Alternate rerank endpoint
                          - match:
                              prefix: "/rerank"
                            route:
                              cluster: llama_rerank
                              timeout: 60s

                          # Chat/Completions → chat backend
                          - match:
                              prefix: "/v1/chat"
                            route:
                              cluster: llama_chat
                              timeout: 600s

                          # Completions (non-chat)
                          - match:
                              prefix: "/v1/completions"
                            route:
                              cluster: llama_chat
                              timeout: 600s

                          # Models list
                          - match:
                              prefix: "/v1/models"
                            route:
                              cluster: llama_chat
                              timeout: 10s

                          # Health checks
                          - match:
                              prefix: "/health"
                            route:
                              cluster: llama_chat
                              timeout: 5s

                          # Default catch-all
                          - match:
                              prefix: "/"
                            route:
                              cluster: llama_chat
                              timeout: 600s

              clusters:
              # Chat/completions backend
              - name: llama_chat
                type: STATIC
                connect_timeout: 5s
                lb_policy: ROUND_ROBIN
                load_assignment:
                  cluster_name: llama_chat
                  endpoints:
                  - lb_endpoints:
                    - endpoint:
                        address:
                          socket_address:
                            address: 127.0.0.1
                            port_value: ${toString cfg.models.chat.port}

              # Embedding model backend
              - name: llama_embed
                type: STATIC
                connect_timeout: 5s
                lb_policy: ROUND_ROBIN
                load_assignment:
                  cluster_name: llama_embed
                  endpoints:
                  - lb_endpoints:
                    - endpoint:
                        address:
                          socket_address:
                            address: 127.0.0.1
                            port_value: ${toString cfg.models.embed.port}

              # Reranking model backend
              - name: llama_rerank
                type: STATIC
                connect_timeout: 5s
                lb_policy: ROUND_ROBIN
                load_assignment:
                  cluster_name: llama_rerank
                  endpoints:
                  - lb_endpoints:
                    - endpoint:
                        address:
                          socket_address:
                            address: 127.0.0.1
                            port_value: ${toString cfg.models.rerank.port}
          '';

        # Envoy gateway start script
        envoyScript = pkgs.writeShellScriptBin "llama-envoy" ''
          #!/usr/bin/env bash
          set -euo pipefail

          CONTAINER_NAME="envoy-llm"
          IMAGE="envoyproxy/envoy:v1.32-latest"
          ENVOY_PORT="${"$"}{ENVOY_PORT:-${toString defaultConfig.envoy.port}}"
          ADMIN_PORT="${"$"}{ADMIN_PORT:-${toString defaultConfig.envoy.adminPort}}"
          CONFIG_FILE="${"$"}{CONFIG_FILE:-configs/envoy.yaml}"
          SCRIPT_DIR="$(cd "$(dirname "$0")" && pwd)"

          # Detect if running from flake or project directory
          if [[ -f "$CONFIG_FILE" ]]; then
              CONFIG_PATH="$(realpath "$CONFIG_FILE")"
          elif [[ -f "$SCRIPT_DIR/../configs/envoy.yaml" ]]; then
              CONFIG_PATH="$(realpath "$SCRIPT_DIR/../configs/envoy.yaml")"
          else
              echo "Error: Cannot find envoy.yaml config file"
              echo "Expected at: $CONFIG_FILE or $SCRIPT_DIR/../configs/envoy.yaml"
              exit 1
          fi

          RED='\033[0;31m'
          GREEN='\033[0;32m'
          YELLOW='\033[1;33m'
          BLUE='\033[0;34m'
          NC='\033[0m'

          log_info() { echo -e "${"$"}{BLUE}[INFO]${"$"}{NC} $1"; }
          log_success() { echo -e "${"$"}{GREEN}[OK]${"$"}{NC} $1"; }
          log_warn() { echo -e "${"$"}{YELLOW}[WARN]${"$"}{NC} $1"; }
          log_error() { echo -e "${"$"}{RED}[ERROR]${"$"}{NC} $1"; }

          check_docker() {
              if ! command -v docker &>/dev/null; then
                  log_error "Docker is not installed"
                  exit 1
              fi
          }

          start_envoy() {
              check_docker

              if docker ps --format '{{.Names}}' | grep -q "^${"$"}{CONTAINER_NAME}$"; then
                  log_warn "Envoy gateway is already running"
                  show_endpoints
                  return 0
              fi

              # Clean up any stopped container
              docker rm "${"$"}{CONTAINER_NAME}" 2>/dev/null || true

              log_info "Starting Envoy LLM gateway..."
              log_info "Config: $CONFIG_PATH"

              docker run -d --rm \
                  --name "${"$"}{CONTAINER_NAME}" \
                  --network host \
                  --user root \
                  -v "${"$"}{CONFIG_PATH}":/etc/envoy/envoy.yaml:ro \
                  "${"$"}{IMAGE}" \
                  -c /etc/envoy/envoy.yaml >/dev/null

              sleep 2

              # Verify startup
              if curl -s "http://localhost:${"$"}{ADMIN_PORT}/ready" | grep -q "LIVE"; then
                  log_success "Envoy gateway started successfully"
                  show_endpoints
              else
                  log_error "Envoy failed to start. Check logs with: $0 logs"
                  exit 1
              fi
          }

          stop_envoy() {
              check_docker
              if docker ps --format '{{.Names}}' | grep -q "^${"$"}{CONTAINER_NAME}$"; then
                  log_info "Stopping Envoy gateway..."
                  docker stop "${"$"}{CONTAINER_NAME}" >/dev/null
                  log_success "Stopped"
              else
                  log_warn "Envoy gateway is not running"
              fi
          }

          show_status() {
              check_docker
              if docker ps --format '{{.Names}}' | grep -q "^${"$"}{CONTAINER_NAME}$"; then
                  echo -e "Status: ${"$"}{GREEN}Running${"$"}{NC}"
                  show_endpoints
                  echo ""
                  log_info "Admin: http://localhost:${"$"}{ADMIN_PORT}"
              else
                  echo -e "Status: ${"$"}{RED}Stopped${"$"}{NC}"
              fi
          }

          show_endpoints() {
              echo ""
              log_info "Unified LLM Gateway: http://localhost:${"$"}{ENVOY_PORT}"
              echo "  Routes:"
              echo "    /v1/chat/completions  → Chat (port ${toString defaultConfig.models.chat.port})"
              echo "    /v1/embeddings        → Embeddings (port ${toString defaultConfig.models.embed.port})"
              echo "    /v1/rerank            → Reranker (port ${toString defaultConfig.models.rerank.port})"
              echo "    /health               → Health check"
          }

          test_endpoints() {
              log_info "Testing Envoy gateway endpoints..."
              echo ""

              # Health
              echo -n "  Health:     "
              if curl -s "http://localhost:${"$"}{ENVOY_PORT}/health" | grep -q "ok"; then
                  echo -e "${"$"}{GREEN}OK${"$"}{NC}"
              else
                  echo -e "${"$"}{RED}FAIL${"$"}{NC}"
              fi

              # Chat
              echo -n "  Chat:       "
              if curl -s "http://localhost:${"$"}{ENVOY_PORT}/v1/models" | grep -q "model"; then
                  echo -e "${"$"}{GREEN}OK${"$"}{NC}"
              else
                  echo -e "${"$"}{YELLOW}Backend not running${"$"}{NC}"
              fi

              # Embeddings
              echo -n "  Embeddings: "
              if curl -s "http://localhost:${toString defaultConfig.models.embed.port}/health" | grep -q "ok"; then
                  echo -e "${"$"}{GREEN}OK${"$"}{NC}"
              else
                  echo -e "${"$"}{YELLOW}Backend not running${"$"}{NC}"
              fi

              # Rerank
              echo -n "  Reranker:   "
              if curl -s "http://localhost:${toString defaultConfig.models.rerank.port}/health" | grep -q "ok"; then
                  echo -e "${"$"}{GREEN}OK${"$"}{NC}"
              else
                  echo -e "${"$"}{YELLOW}Backend not running${"$"}{NC}"
              fi
          }

          case "${"$"}{1:-start}" in
              start) start_envoy ;;
              stop) stop_envoy ;;
              restart) stop_envoy; sleep 1; start_envoy ;;
              status) show_status ;;
              test) test_endpoints ;;
              logs) docker logs -f "${"$"}{CONTAINER_NAME}" ;;
              *)
                  echo "Envoy LLM Gateway Manager"
                  echo ""
                  echo "Usage: $0 [command]"
                  echo ""
                  echo "Commands:"
                  echo "  start    Start the Envoy gateway (default)"
                  echo "  stop     Stop the Envoy gateway"
                  echo "  restart  Restart the Envoy gateway"
                  echo "  status   Show gateway status and endpoints"
                  echo "  test     Test all backend endpoints"
                  echo "  logs     Follow container logs"
                  echo ""
                  echo "Environment:"
                  echo "  ENVOY_PORT   Gateway port (default: ${toString defaultConfig.envoy.port})"
                  echo "  ADMIN_PORT   Admin port (default: ${toString defaultConfig.envoy.adminPort})"
                  echo "  CONFIG_FILE  Config path (default: configs/envoy.yaml)"
                  exit 1
                  ;;
          esac
        '';

        # Config generation script using llm-config.nix
        generateConfigsScript = pkgs.writeShellScriptBin "llama-generate-configs" ''
          #!/usr/bin/env bash
          set -euo pipefail

          RED='\033[0;31m'
          GREEN='\033[0;32m'
          YELLOW='\033[1;33m'
          BLUE='\033[0;34m'
          NC='\033[0m'

          log_info() { echo -e "${"$"}{BLUE}[INFO]${"$"}{NC} $1"; }
          log_success() { echo -e "${"$"}{GREEN}[OK]${"$"}{NC} $1"; }
          log_warn() { echo -e "${"$"}{YELLOW}[WARN]${"$"}{NC} $1"; }

          OUTPUT_DIR="${"$"}{OUTPUT_DIR:-configs/generated}"
          INSTALL_DIR="${"$"}{INSTALL_DIR:-/etc/llama-server}"

          show_config() {
              log_info "Current LLM Configuration:"
              echo ""
              echo "  Hardware:  ${llmConfig.activeConfig.hardware.name}"
              echo "  GPU Arch:  ${llmConfig.activeConfig.hardware.gpuArch or "N/A"}"
              echo ""
              echo "  Services:"
              echo "    Chat:      ${llmConfig.activeConfig.services.chat.model.displayName} (port ${toString llmConfig.activeConfig.services.chat.endpoint.port})"
              echo "    Embedding: ${llmConfig.activeConfig.services.embedding.model.displayName} (port ${toString llmConfig.activeConfig.services.embedding.endpoint.port})"
              echo "    Reranking: ${llmConfig.activeConfig.services.reranking.model.displayName} (port ${toString llmConfig.activeConfig.services.reranking.endpoint.port})"
              echo ""
              echo "  Gateway:   http://localhost:${toString llmConfig.activeConfig.gateway.port}"
              echo ""
              echo "  OpenAI Aliases:"
              echo "    Chat:       ${
                builtins.concatStringsSep ", " (
                  llmConfig.activeConfig.services.chat.endpoint.aliases
                  ++ llmConfig.activeConfig.services.chat.modelAliases
                )
              }"
              echo "    Embeddings: ${
                builtins.concatStringsSep ", " (
                  llmConfig.activeConfig.services.embedding.endpoint.aliases
                  ++ llmConfig.activeConfig.services.embedding.modelAliases
                )
              }"
              echo "    Reranking:  ${
                builtins.concatStringsSep ", " (
                  llmConfig.activeConfig.services.reranking.endpoint.aliases
                  ++ llmConfig.activeConfig.services.reranking.modelAliases
                )
              }"
          }

          generate_configs() {
              log_info "Generating configurations to $OUTPUT_DIR"
              mkdir -p "$OUTPUT_DIR"

              # Server configs (only for enabled services)
              log_info "Generating server configs..."
              ${
                if llmConfig.activeConfig.enable.chat then
                  ''
                        cat > "$OUTPUT_DIR/chat.conf" << 'CONF'
                    ${llmConfig.serverConfigs.chat}
                    CONF
                        log_success "chat.conf"
                  ''
                else
                  ''
                    log_warn "chat service disabled, skipping"
                  ''
              }

              ${
                if llmConfig.activeConfig.enable.embedding then
                  ''
                        cat > "$OUTPUT_DIR/embedding.conf" << 'CONF'
                    ${llmConfig.serverConfigs.embedding}
                    CONF
                        log_success "embedding.conf"
                  ''
                else
                  ''
                    log_warn "embedding service disabled, skipping"
                  ''
              }

              ${
                if llmConfig.activeConfig.enable.reranking then
                  ''
                        cat > "$OUTPUT_DIR/reranking.conf" << 'CONF'
                    ${llmConfig.serverConfigs.reranking}
                    CONF
                        log_success "reranking.conf"
                  ''
                else
                  ''
                    log_warn "reranking service disabled, skipping"
                  ''
              }

              # Gateway configs
              ${
                if llmConfig.activeConfig.enable.envoy then
                  ''
                        log_info "Generating Envoy gateway config..."
                        cat > "$OUTPUT_DIR/envoy.yaml" << 'YAML'
                    ${llmConfig.envoyConfig}
                    YAML
                        log_success "envoy.yaml"
                  ''
                else
                  ''
                    log_warn "envoy gateway disabled, skipping"
                  ''
              }

              ${
                if llmConfig.activeConfig.enable.aigw then
                  ''
                        log_info "Generating AI Gateway config..."
                        cat > "$OUTPUT_DIR/aigw-config.yaml" << 'YAML'
                    ${llmConfig.aigwConfig}
                    YAML
                        log_success "aigw-config.yaml"
                  ''
                else
                  ""
              }

              ${
                if llmConfig.activeConfig.enable.litellm then
                  ''
                        log_info "Generating LiteLLM config..."
                        cat > "$OUTPUT_DIR/litellm-config.yaml" << 'YAML'
                    ${llmConfig.litellmConfig}
                    YAML
                        log_success "litellm-config.yaml"
                  ''
                else
                  ""
              }

              # Systemd services (only for enabled services)
              log_info "Generating systemd services..."
              ${
                if llmConfig.activeConfig.enable.chat then
                  ''
                        cat > "$OUTPUT_DIR/llama-server-chat.service" << 'SERVICE'
                    ${llmConfig.systemdServices.chat}
                    SERVICE
                        log_success "llama-server-chat.service"
                  ''
                else
                  ""
              }

              ${
                if llmConfig.activeConfig.enable.embedding then
                  ''
                        cat > "$OUTPUT_DIR/llama-server-embedding.service" << 'SERVICE'
                    ${llmConfig.systemdServices.embedding}
                    SERVICE
                        log_success "llama-server-embedding.service"
                  ''
                else
                  ""
              }

              ${
                if llmConfig.activeConfig.enable.reranking then
                  ''
                        cat > "$OUTPUT_DIR/llama-server-reranking.service" << 'SERVICE'
                    ${llmConfig.systemdServices.reranking}
                    SERVICE
                        log_success "llama-server-reranking.service"
                  ''
                else
                  ""
              }

              # Documentation
              log_info "Generating documentation..."
              cat > "$OUTPUT_DIR/CONFIGURATION.md" << 'MD'
          ${llmConfig.documentation}
          MD
              log_success "CONFIGURATION.md"

              echo ""
              log_success "All configs generated in $OUTPUT_DIR"
              echo ""
              echo "Files generated:"
              ls -la "$OUTPUT_DIR"
          }

          install_configs() {
              log_info "Installing configurations..."

              if [[ ! -d "$OUTPUT_DIR" ]]; then
                  log_warn "No generated configs found. Running generate first..."
                  generate_configs
              fi

              log_info "Installing server configs to $INSTALL_DIR"
              sudo mkdir -p "$INSTALL_DIR"
              sudo cp "$OUTPUT_DIR"/*.conf "$INSTALL_DIR/" 2>/dev/null || true
              log_success "Server configs installed"

              log_info "Installing Envoy config to configs/"
              cp "$OUTPUT_DIR/envoy.yaml" configs/envoy.yaml
              log_success "Envoy config updated"

              log_info "Installing LiteLLM config to configs/"
              cp "$OUTPUT_DIR/litellm-config.yaml" configs/litellm-config.yaml
              log_success "LiteLLM config updated"

              log_info "Installing systemd services..."
              sudo cp "$OUTPUT_DIR"/*.service /etc/systemd/system/ 2>/dev/null || true
              sudo systemctl daemon-reload
              log_success "Systemd services installed"

              echo ""
              log_success "Installation complete!"
              echo ""
              echo "Start services with:"
              echo "  sudo systemctl start llama-server-chat"
              echo "  sudo systemctl start llama-server-embedding"
              echo "  sudo systemctl start llama-server-reranking"
              echo "  nix run .#envoy start"
          }

          case "${"$"}{1:-show}" in
              show|info)
                  show_config
                  ;;
              generate|gen)
                  generate_configs
                  ;;
              install)
                  install_configs
                  ;;
              all)
                  show_config
                  echo ""
                  generate_configs
                  ;;
              *)
                  echo "LLM Configuration Generator"
                  echo ""
                  echo "Usage: $0 [command]"
                  echo ""
                  echo "Commands:"
                  echo "  show      Show current configuration (default)"
                  echo "  generate  Generate all config files to configs/generated/"
                  echo "  install   Install configs to system locations"
                  echo "  all       Show config and generate files"
                  echo ""
                  echo "Environment:"
                  echo "  OUTPUT_DIR   Output directory (default: configs/generated)"
                  echo "  INSTALL_DIR  System config dir (default: /etc/llama-server)"
                  exit 1
                  ;;
          esac
        '';

      in
      {
        # Development shell
        devShells.default = pkgs.mkShell {
          name = "local-llama-dev";

          buildInputs = with pkgs; [
            # Python for LiteLLM
            (python311.withPackages (
              ps: with ps; [
                litellm
                redis
                uvicorn
                fastapi
              ]
            ))

            # Tools
            jq
            curl
            httpie

            # Nix tools
            nil # Nix LSP
            nixpkgs-fmt
          ];

          shellHook = ''
            echo "Local LLama Development Environment"
            echo ""
            echo "Available commands:"
            echo "  nix run .#generate-configs  - Generate/show LLM configurations"
            echo "  nix run .#build             - Build llama.cpp (rocm/vulkan/cpu)"
            echo "  nix run .#download          - Download models from HuggingFace"
            echo "  nix run .#envoy             - Start Envoy LLM gateway (start/stop/status/test)"
            echo "  nix run .#install           - Install systemd services"
            echo "  nix run .#litellm           - Start LiteLLM AI proxy"
            echo "  nix run .#firewall          - Manage firewall rules (enable/disable/status)"
            echo "  nix run .#webui             - Deploy Open WebUI chat interface (start/stop/status)"
            echo ""
            echo "Active Configuration (from nix/llm-config.nix):"
            echo "  Chat:      ${llmConfig.activeConfig.services.chat.model.displayName} (port ${toString llmConfig.activeConfig.services.chat.endpoint.port})"
            echo "  Embedding: ${llmConfig.activeConfig.services.embedding.model.displayName} (port ${toString llmConfig.activeConfig.services.embedding.endpoint.port})"
            echo "  Reranking: ${llmConfig.activeConfig.services.reranking.model.displayName} (port ${toString llmConfig.activeConfig.services.reranking.endpoint.port})"
            echo "  Gateway:   http://localhost:${toString llmConfig.activeConfig.gateway.port}"
            echo ""
            echo "ROCm environment configured for ${defaultConfig.rocm.gpuTarget}"

            ${builtins.concatStringsSep "\n" (
              builtins.attrValues (builtins.mapAttrs (k: v: "export ${k}=\"${v}\"") rocmEnv)
            )}
          '';
        };

        # Packages
        packages = {
          install-script = installScript;
          litellm-script = litellmScript;
          firewall-script = firewallScript;
          webui-script = webuiScript;
          envoy-script = envoyScript;
          generate-configs-script = generateConfigsScript;
          build-llama-script = buildLlamaScript;
          download-model-script = downloadModelScript;
          litellm-config = makeLiteLLMConfig defaultConfig;
          envoy-config = makeEnvoyConfig defaultConfig;

          # Configs from llm-config.nix module
          llm-envoy-config = llmConfig.packages.envoyConfigFile;
          llm-litellm-config = llmConfig.packages.litellmConfigFile;
          llm-aigw-config = llmConfig.packages.aigwConfigFile;
          llm-documentation = llmConfig.packages.documentationFile;

          default = installScript;
        };

        # Apps
        apps = {
          install = {
            type = "app";
            program = "${installScript}/bin/install-llama-services";
          };

          litellm = {
            type = "app";
            program = "${litellmScript}/bin/start-litellm-proxy";
          };

          firewall = {
            type = "app";
            program = "${firewallScript}/bin/llama-firewall";
          };

          webui = {
            type = "app";
            program = "${webuiScript}/bin/llama-webui";
          };

          envoy = {
            type = "app";
            program = "${envoyScript}/bin/llama-envoy";
          };

          generate-configs = {
            type = "app";
            program = "${generateConfigsScript}/bin/llama-generate-configs";
          };

          build = {
            type = "app";
            program = "${buildLlamaScript}/bin/llama-build";
          };

          download = {
            type = "app";
            program = "${downloadModelScript}/bin/llama-download";
          };

          default = self.apps.${system}.install;
        };
      }
    )
    // {
      # NixOS module (system-wide)
      nixosModules.default =
        {
          config,
          lib,
          pkgs,
          ...
        }:
        with lib;
        let
          cfg = config.services.llama-server;
        in
        {
          options.services.llama-server = {
            enable = mkEnableOption "Local LLM server infrastructure";

            user = mkOption {
              type = types.str;
              default = "llama";
              description = "User to run llama-server as";
            };

            group = mkOption {
              type = types.str;
              default = "llama";
              description = "Group to run llama-server as";
            };

            modelsDir = mkOption {
              type = types.path;
              default = "/var/lib/llama/models";
              description = "Directory containing GGUF model files";
            };

            llamaCppPath = mkOption {
              type = types.path;
              default = "/opt/llama.cpp";
              description = "Path to llama.cpp installation";
            };

            rocm = {
              enable = mkEnableOption "ROCm/HIP acceleration";

              gpuTarget = mkOption {
                type = types.str;
                default = "gfx1151";
                description = "GPU architecture target";
              };

              umaEnabled = mkOption {
                type = types.bool;
                default = false;
                description = "Enable Unified Memory Architecture for APUs";
              };
            };

            models = mkOption {
              type = types.attrsOf (
                types.submodule {
                  options = {
                    modelFile = mkOption {
                      type = types.str;
                      description = "GGUF model filename";
                    };

                    port = mkOption {
                      type = types.port;
                      description = "Port to serve on";
                    };

                    contextSize = mkOption {
                      type = types.int;
                      default = 8192;
                      description = "Context window size";
                    };

                    parallelSlots = mkOption {
                      type = types.int;
                      default = 1;
                      description = "Number of parallel request slots";
                    };

                    extraFlags = mkOption {
                      type = types.listOf types.str;
                      default = [ ];
                      description = "Additional flags for llama-server";
                    };

                    mode = mkOption {
                      type = types.enum [
                        "chat"
                        "embedding"
                        "reranking"
                      ];
                      default = "chat";
                      description = "Server mode";
                    };
                  };
                }
              );
              default = { };
              description = "Model configurations";
            };

            proxy = {
              type = mkOption {
                type = types.enum [
                  "nginx"
                  "litellm"
                  "none"
                ];
                default = "litellm";
                description = "Proxy type to use";
              };

              port = mkOption {
                type = types.port;
                default = 4000;
                description = "Proxy port";
              };

              tls = {
                enable = mkEnableOption "TLS encryption";

                certFile = mkOption {
                  type = types.nullOr types.path;
                  default = null;
                  description = "Path to TLS certificate";
                };

                keyFile = mkOption {
                  type = types.nullOr types.path;
                  default = null;
                  description = "Path to TLS private key";
                };
              };

              cache = {
                enable = mkOption {
                  type = types.bool;
                  default = true;
                  description = "Enable response caching (recommended for embeddings)";
                };

                type = mkOption {
                  type = types.enum [
                    "local"
                    "redis"
                  ];
                  default = "local";
                  description = "Cache backend type";
                };

                ttl = mkOption {
                  type = types.int;
                  default = 3600;
                  description = "Cache TTL in seconds";
                };
              };
            };
          };

          config = mkIf cfg.enable {
            # Implementation would go here for full NixOS integration
            # This is a template for the module structure
            warnings = [
              "llama-server NixOS module is a template - see flake for full implementation"
            ];
          };
        };

      # Home-manager module (user-level)
      homeManagerModules.default =
        {
          config,
          lib,
          pkgs,
          ...
        }:
        with lib;
        let
          cfg = config.services.llama-server;
        in
        {
          options.services.llama-server = {
            enable = mkEnableOption "Local LLM server (user services)";
            # Similar options as NixOS module but for user systemd
          };

          config = mkIf cfg.enable {
            warnings = [
              "llama-server home-manager module is a template - see flake for full implementation"
            ];
          };
        };
    };
}
