# Configuration for Devstral-2-123B
# Generated by nix/llm-config.nix
# Service: llama-server@chat

# Binary path
LLAMA_BIN=$HOME/llama.cpp/build-rocm/bin/llama-server

# Model path
MODEL_PATH=$HOME/models/Q4_K_M/Devstral-2-123B-Instruct-2512-Q4_K_M-00001-of-00002.gguf

# Server settings
HOST=0.0.0.0
PORT=8000

# Context and GPU
CTX_SIZE=262144
N_GPU_LAYERS=999

# Mode-specific and hardware flags
EXTRA_FLAGS=--flash-attn on --cache-type-k q8_0 --cache-type-v q8_0 --no-mmap -fit off --n-gpu-layers 999 --parallel 1 --batch-size 4096 --ubatch-size 1024 --threads 16 --threads-batch 16 --cont-batching

