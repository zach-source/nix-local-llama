[Unit]
Description=LLaMA Server (embedding - Qwen3-Embedding-8B)
Documentation=https://github.com/ggerganov/llama.cpp
After=network.target
Wants=network-online.target

[Service]
Type=simple
User=ztaylor
Group=ztaylor

Environment="GGML_CUDA_ENABLE_UNIFIED_MEMORY=1"
Environment="GPU_MAX_ALLOC_PERCENT=99"
Environment="GPU_MAX_HEAP_SIZE=99"
Environment="GPU_MAX_HW_QUEUES=8"
Environment="HIP_VISIBLE_DEVICES=0"
Environment="HSA_ENABLE_SDMA=0"
Environment="HSA_OVERRIDE_GFX_VERSION=11.5.1"
Environment="LD_LIBRARY_PATH=/opt/rocm/lib:/usr/lib/x86_64-linux-gnu"
Environment="ROCBLAS_USE_HIPBLASLT=1"

EnvironmentFile=/etc/llama-server/embedding.conf
Environment="EXTRA_FLAGS="

ExecStart=/bin/bash -c "${LLAMA_BIN} --model ${MODEL_PATH} --host ${HOST} --port ${PORT} --ctx-size ${CTX_SIZE} --n-gpu-layers ${N_GPU_LAYERS} ${EXTRA_FLAGS}"

Restart=on-failure
RestartSec=10
StartLimitBurst=3
StartLimitIntervalSec=60

LimitNOFILE=65536
LimitMEMLOCK=infinity

StandardOutput=journal
StandardError=journal
SyslogIdentifier=llama-server-embedding

[Install]
WantedBy=multi-user.target

