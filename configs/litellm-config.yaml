# LiteLLM Proxy Configuration
# AI-aware proxy with caching, routing, and OpenAI API compatibility
#
# Start with: litellm --config configs/litellm-config.yaml --port 4000
# Or use: nix run .#litellm

model_list:
  # ============================================================================
  # Chat/Completions Models
  # Routes to Qwen3-Coder-30B-A3B on port 8000
  # ============================================================================

  # OpenAI aliases - allows tools expecting OpenAI to work seamlessly
  - model_name: gpt-4
    litellm_params:
      model: openai/qwen3-coder
      api_base: http://localhost:8000/v1
      api_key: "sk-local"

  - model_name: gpt-4-turbo
    litellm_params:
      model: openai/qwen3-coder
      api_base: http://localhost:8000/v1
      api_key: "sk-local"

  - model_name: gpt-4o
    litellm_params:
      model: openai/qwen3-coder
      api_base: http://localhost:8000/v1
      api_key: "sk-local"

  - model_name: gpt-3.5-turbo
    litellm_params:
      model: openai/qwen3-coder
      api_base: http://localhost:8000/v1
      api_key: "sk-local"

  # Claude aliases
  - model_name: claude-3-opus
    litellm_params:
      model: openai/qwen3-coder
      api_base: http://localhost:8000/v1
      api_key: "sk-local"

  - model_name: claude-3-sonnet
    litellm_params:
      model: openai/qwen3-coder
      api_base: http://localhost:8000/v1
      api_key: "sk-local"

  # Direct model name
  - model_name: qwen3-coder
    litellm_params:
      model: openai/qwen3-coder
      api_base: http://localhost:8000/v1
      api_key: "sk-local"

  # ============================================================================
  # Embedding and Reranker Models
  #
  # NOTE: LiteLLM has compatibility issues with llama.cpp embeddings/reranking API.
  # The OpenAI SDK provider strips the model name from requests, causing errors.
  #
  # Access these endpoints directly instead:
  #   Embeddings: http://localhost:8001/v1/embeddings
  #   Reranker:   http://localhost:8002/v1/rerank
  #
  # Both support OpenAI-compatible API format with any model name.
  # ============================================================================

# ==============================================================================
# LiteLLM Settings
# ==============================================================================
litellm_settings:
  # Caching - Critical for embeddings performance
  # Same input text always produces same embedding vector
  cache: true
  cache_params:
    type: "local"           # "redis" for persistent/distributed cache
    ttl: 3600               # Cache TTL in seconds (1 hour)
    # For Redis:
    # type: "redis"
    # host: "localhost"
    # port: 6379

  # Timeouts (in seconds)
  request_timeout: 600      # 10 minutes for long generations

  # Retries
  num_retries: 2

  # Streaming
  set_verbose: false

  # Callbacks for logging/monitoring
  success_callback: []
  failure_callback: []

# ==============================================================================
# General Settings
# ==============================================================================
general_settings:
  # Master API key for admin operations
  # Change this in production!
  master_key: "sk-local-llm-master"

  # Database for usage tracking (optional)
  # database_url: "postgresql://user:pass@localhost/litellm"

  # Enable detailed request logging
  # store_model_in_db: true
