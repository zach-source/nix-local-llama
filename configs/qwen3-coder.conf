# Configuration for Qwen3-Coder-30B-A3B model
# Copy to: /etc/llama-server/qwen3-coder.conf
# Used with: systemctl start llama-server@qwen3-coder

# Binary path
LLAMA_BIN=/home/ztaylor/llama.cpp/build-rocm/bin/llama-server

# Model path
MODEL_PATH=/home/ztaylor/models/Qwen3-Coder-30B-A3B-Instruct-Q6_K.gguf

# Server settings
HOST=0.0.0.0
PORT=8000

# Context and GPU
CTX_SIZE=262144
N_GPU_LAYERS=999

# Additional flags for UMA/Strix Halo
EXTRA_FLAGS=--parallel 1 --flash-attn on --cache-type-k q8_0 --cache-type-v q8_0 --no-mmap -fit off --batch-size 4096 --ubatch-size 1024 --threads 16 --threads-batch 16 --cont-batching
