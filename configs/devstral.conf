# Configuration for Devstral-24B model
# Copy to: /etc/llama-server/devstral.conf
# Used with: systemctl start llama-server@devstral

# Binary path
LLAMA_BIN=/opt/llama.cpp/build-uma/bin/llama-server

# Model path
MODEL_PATH=/models/Devstral-Small-2-24B-Instruct-2512-Q4_K_M.gguf

# Server settings
HOST=0.0.0.0
PORT=8000

# Context and GPU
CTX_SIZE=131072
N_GPU_LAYERS=999

# API key (optional)
# API_KEY_FILE=/etc/llama-server/api.key
